{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Network\n",
    "class MappingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Mapping Network class for mapping latent vectors to intermediate latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers):\n",
    "        super(MappingNetwork, self).__init__()\n",
    "        layers = [nn.Linear(latent_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.ReLU()])\n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mapping(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Injection\n",
    "class NoiseInjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Noise Injection class for adding random noise to feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(NoiseInjection, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        noise = torch.randn_like(x)\n",
    "        return x + self.scale * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Instance Normalization (AdaIN)\n",
    "class AdaIN(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Instance Normalization (AdaIN) class for applying style to feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(AdaIN, self).__init__()\n",
    "        self.norm = nn.InstanceNorm2d(channels)\n",
    "        self.style = nn.Linear(latent_dim, channels * 2)\n",
    "    \n",
    "    def forward(self, x, w):\n",
    "        style = self.style(w).unsqueeze(-1).unsqueeze(-1)\n",
    "        gamma, beta = style.chunk(2, 1)\n",
    "        return (1 + gamma) * self.norm(x) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention class for applying self-attention to feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        query = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        key = self.key(x).view(batch_size, -1, width * height)\n",
    "        attention = torch.bmm(query, key)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        value = self.value(x).view(batch_size, -1, width * height)\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, channels, height, width)\n",
    "        return self.gamma * out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blur\n",
    "class Blur(nn.Module):\n",
    "    \"\"\"\n",
    "    Blur class for applying blurring to feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(Blur, self).__init__()\n",
    "        kernel = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\n",
    "        kernel = kernel[None, None, :, :] / kernel.sum()\n",
    "        self.register_buffer('kernel', kernel.repeat(channels, 1, 1, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, self.kernel, stride=1, padding=1, groups=x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style Layer\n",
    "class StyleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Style Layer class for applying style and upsampling/downsampling to feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, in_channels, out_channels, kernel_size=3, upsample=False, attention=False):\n",
    "        super(StyleLayer, self).__init__()\n",
    "        self.noise_injection = NoiseInjection()\n",
    "        self.adain = AdaIN(latent_dim, out_channels)\n",
    "        self.conv = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2))\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.blur = Blur(out_channels)\n",
    "        self.attention = SelfAttention(out_channels) if attention else None\n",
    "    \n",
    "    def forward(self, x, w):\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            x = self.blur(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.noise_injection(x)\n",
    "        x = self.adain(x, w)\n",
    "        x = self.act(x)\n",
    "        if self.attention:\n",
    "            x = self.attention(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block class for applying residual connections to feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, 3, padding=1))\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, 3, padding=1))\n",
    "        self.downsample = downsample\n",
    "        self.downsample_layer = nn.Conv2d(in_channels, out_channels, 1, stride=2) if downsample else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.downsample:\n",
    "            x = F.avg_pool2d(x, 2)\n",
    "            residual = self.do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator with Prompt Conditioning\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator class for generating images from latent vectors and prompts.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, output_channels, num_layers, clip_model=None):\n",
    "        super(Generator, self).__init__()\n",
    "        self.mapping = MappingNetwork(latent_dim, hidden_dim, num_layers)\n",
    "        self.style_layers = nn.ModuleList([\n",
    "            StyleLayer(hidden_dim, hidden_dim, hidden_dim, upsample=True),\n",
    "            StyleLayer(hidden_dim, hidden_dim, hidden_dim, upsample=True, attention=True),\n",
    "            StyleLayer(hidden_dim, hidden_dim, hidden_dim, upsample=True),\n",
    "            StyleLayer(hidden_dim, hidden_dim, hidden_dim, upsample=True),\n",
    "            StyleLayer(hidden_dim, hidden_dim, hidden_dim, upsample=True),\n",
    "            StyleLayer(hidden_dim, hidden_dim, hidden_dim, upsample=True, attention=True),\n",
    "            StyleLayer(hidden_dim, hidden_dim, hidden_dim, upsample=True),\n",
    "            StyleLayer(hidden_dim, hidden_dim, hidden_dim, upsample=True),\n",
    "            StyleLayer(hidden_dim, hidden_dim, output_channels, upsample=False)\n",
    "        ])\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            nn.Conv2d(output_channels, output_channels, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.clip_model = clip_model\n",
    "    \n",
    "    def forward(self, z, prompt=None):\n",
    "        w = self.mapping(z)\n",
    "        w = w.unsqueeze(1).repeat(1, len(self.style_layers), 1)\n",
    "        x = torch.randn(z.shape[0], self.style_layers[0].conv.in_channels, 4, 4).to(z.device)\n",
    "        for i, style_layer in enumerate(self.style_layers):\n",
    "            x = style_layer(x, w[:, i])\n",
    "        x = self.to_rgb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator class for determining the realness of generated images.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, hidden_dim, num_layers):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.utils.spectral_norm(nn.Conv2d(input_channels, hidden_dim, 4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        ])\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.extend([\n",
    "                ResidualBlock(hidden_dim, hidden_dim * 2, downsample=True),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            ])\n",
    "            hidden_dim *= 2\n",
    "        \n",
    "        self.layers.extend([\n",
    "            nn.Conv2d(hidden_dim, 1, 4, stride=1, padding=0),\n",
    "            nn.Flatten()\n",
    "        ])\n",
    "        \n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with gradient penalty, path length regularization, and style mixing regularization\n",
    "def train(generator, discriminator, dataloader, num_epochs, latent_dim, device):\n",
    "    \"\"\"\n",
    "    Training loop for the GAN.\n",
    "    \"\"\"\n",
    "    g_optim = optim.Adam(generator.parameters(), lr=0.001, betas=(0.0, 0.99))\n",
    "    d_optim = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.0, 0.99))\n",
    "    \n",
    "    g_ema = ExponentialMovingAverage(generator.parameters(), decay=0.999)\n",
    "    \n",
    "    # Use hinge loss for improved training stability\n",
    "    def d_loss_fn(real_scores, fake_scores):\n",
    "        return F.relu(1.0 - real_scores).mean() + F.relu(1.0 + fake_scores).mean()\n",
    "    \n",
    "    def g_loss_fn(fake_scores):\n",
    "        return -fake_scores.mean()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for real_images in dataloader:\n",
    "            real_images = real_images.to(device)\n",
    "            batch_size = real_images.shape[0]\n",
    "            \n",
    "            # Train discriminator\n",
    "            d_optim.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_images = generator(z)\n",
    "            \n",
    "            real_scores = discriminator(real_images)\n",
    "            fake_scores = discriminator(fake_images.detach())\n",
    "            \n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_images, fake_images, device)\n",
    "            \n",
    "            d_loss = d_loss_fn(real_scores, fake_scores) + 10 * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            d_optim.step()\n",
    "            \n",
    "            # Train generator\n",
    "            g_optim.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_images = generator(z)\n",
    "            \n",
    "            fake_scores = discriminator(fake_images)\n",
    "            \n",
    "            path_length_regularization = compute_path_length_regularization(generator, z, fake_images)\n",
    "            style_mixing_regularization = compute_style_mixing_regularization(generator, z)\n",
    "            \n",
    "            g_loss = g_loss_fn(fake_scores) + 2 * path_length_regularization + 2 * style_mixing_regularization\n",
    "            g_loss.backward()\n",
    "            g_optim.step()\n",
    "            \n",
    "            g_ema.update(generator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute gradient penalty for WGAN-GP\n",
    "def compute_gradient_penalty(discriminator, real_images, fake_images, device):\n",
    "    \"\"\"\n",
    "    Computes the gradient penalty for WGAN-GP.\n",
    "    \"\"\"\n",
    "    alpha = torch.rand(real_images.size(0), 1, 1, 1).to(device)\n",
    "    interpolated = (alpha * real_images + (1 - alpha) * fake_images).requires_grad_(True)\n",
    "    \n",
    "    interpolated_scores = discriminator(interpolated)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolated_scores,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones(interpolated_scores.size()).to(device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute path length regularization\n",
    "def compute_path_length_regularization(generator, z, fake_images):\n",
    "    \"\"\"\n",
    "    Computes the path length regularization term for the generator.\n",
    "    \"\"\"\n",
    "    path_lengths = torch.sqrt((fake_images ** 2).sum([2, 3])).mean(1)\n",
    "    return ((path_lengths - path_lengths.mean()) ** 2).mean()\n",
    "\n",
    "# Compute style mixing regularization\n",
    "def compute_style_mixing_regularization(generator, z):\n",
    "    \"\"\"\n",
    "    Computes the style mixing regularization term for the generator.\n",
    "    \"\"\"\n",
    "    z2 = torch.randn_like(z)\n",
    "    fake_images2 = generator(z2)\n",
    "    mixed_fake_images = 0.5 * fake_images + 0.5 * fake_images2\n",
    "    return ((mixed_fake_images - mixed_fake_images.mean()) ** 2).mean()\n",
    "\n",
    "class ExponentialMovingAverage:\n",
    "    \"\"\"\n",
    "    Exponential Moving Average (EMA) class for model parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, decay):\n",
    "        self.parameters = list(parameters)\n",
    "        self.decay = decay\n",
    "        self.shadow_params = [p.clone().detach() for p in self.parameters]\n",
    "\n",
    "    def update(self, parameters):\n",
    "        for shadow_param, param in zip(self.shadow_params, parameters):\n",
    "            shadow_param.data = self.decay * shadow_param.data + (1.0 - self.decay) * param.data\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading dataset: [Errno 20] Not a directory: 'dataset/CAT_06/00001489_028.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pot/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pot/lib/python3.12/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m     class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pot/lib/python3.12/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pot/lib/python3.12/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mSee :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'dataset/CAT_06/00001489_028.jpg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Initialize models and move to device\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading dataset: [Errno 20] Not a directory: 'dataset/CAT_06/00001489_028.jpg'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the dataset path exists\n",
    "dataset_path = 'dataset/CAT_00'\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset path '{dataset_path}' does not exist.\")\n",
    "\n",
    "# Verify the directory structure\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"Checking directory: {root}\")\n",
    "    if not dirs and not files:\n",
    "        raise ValueError(f\"No subdirectories or files found in the dataset path '{dataset_path}'. Ensure the directory structure follows the expected format for ImageFolder.\")\n",
    "    for dir_name in dirs:\n",
    "        print(f\"Found subdirectory: {dir_name}\")\n",
    "    for file_name in files:\n",
    "        print(f\"Found file: {file_name}\")\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "try:\n",
    "    dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    print(f\"Loaded dataset with {len(dataset)} images.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading dataset: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Initialize models and move to device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    latent_dim = 512\n",
    "    generator = Generator(latent_dim, hidden_dim=512, output_channels=3, num_layers=8).to(device)\n",
    "    discriminator = Discriminator(input_channels=3, hidden_dim=64, num_layers=4).to(device)\n",
    "\n",
    "    # Train the GAN\n",
    "    train(generator, discriminator, dataloader, num_epochs=100, latent_dim=latent_dim, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
